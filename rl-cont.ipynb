{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class CartPoleNonStationaryEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.episode_counter = 0\n",
    "        self.gravity_values = [9.8, 15.0, 5.0, 20.0]  # Different gravity settings\n",
    "        self.change_interval = 10  # Change gravity every 10 episodes\n",
    "\n",
    "    def reset(self):\n",
    "        # Change gravity after every 'change_interval' episodes\n",
    "        if self.episode_counter % self.change_interval == 0:\n",
    "            new_gravity = np.random.choice(self.gravity_values)\n",
    "            self.env.env.gravity = new_gravity\n",
    "            print(f\"Gravity changed to: {new_gravity}\")\n",
    "        \n",
    "        self.episode_counter += 1\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        return self.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.softmax(x, dim=-1)\n",
    "\n",
    "def select_action(policy_net, state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy_net(state)\n",
    "    action = torch.multinomial(probs, 1).item()\n",
    "    return action, torch.log(probs[0, action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_continual_backprop(env, policy_net, optimizer, episodes=1000, use_continual=False):\n",
    "    gamma = 0.99\n",
    "    reward_threshold = 195.0\n",
    "    replacement_rate = 0.3 if use_continual else 0\n",
    "    maturity_threshold = 10\n",
    "\n",
    "    utility1, utility2 = torch.zeros(policy_net.fc1.out_features), torch.zeros(policy_net.fc2.out_features)\n",
    "    age1, age2 = torch.zeros(policy_net.fc1.out_features), torch.zeros(policy_net.fc2.out_features)\n",
    "\n",
    "    def replace_features(layer, utility, age, num_replacements):\n",
    "        lowest_util_indices = utility.argsort()[:num_replacements]\n",
    "        for i in lowest_util_indices:\n",
    "            layer.weight.data[i, :] = torch.randn(layer.weight.shape[1]) * 0.01\n",
    "            utility[i] = 0\n",
    "            age[i] = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for t in range(1, 10000):  # Limit to 10000 steps\n",
    "            action, log_prob = select_action(policy_net, state)\n",
    "            state, reward, done, _,  _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Compute discounted rewards\n",
    "        discounted_rewards = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            discounted_rewards.insert(0, R)\n",
    "        discounted_rewards = torch.tensor(discounted_rewards)\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n",
    "\n",
    "        # Update policy\n",
    "        loss = -torch.sum(torch.stack(log_probs) * discounted_rewards)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track and update utility for continual backprop\n",
    "        if use_continual:\n",
    "            with torch.no_grad():\n",
    "                # Compute activations for continual backprop\n",
    "                state = torch.from_numpy(state).float()\n",
    "                hidden_output1 = torch.relu(policy_net.fc1(state))\n",
    "                hidden_output2 = torch.relu(policy_net.fc2(hidden_output1))\n",
    "\n",
    "                # Update utility and age\n",
    "                utility1 += torch.abs(hidden_output1)\n",
    "                utility2 += torch.abs(hidden_output2)\n",
    "                age1 += 1\n",
    "                age2 += 1\n",
    "\n",
    "                # Periodic replacement\n",
    "                if episode % maturity_threshold == 0:\n",
    "                    replace_features(policy_net.fc1, utility1, age1, int(replacement_rate * utility1.size(0)))\n",
    "                    replace_features(policy_net.fc2, utility2, age2, int(replacement_rate * utility2.size(0)))\n",
    "\n",
    "        # Logging\n",
    "        total_reward = sum(rewards)\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}\")\n",
    "        if total_reward >= reward_threshold:\n",
    "            print(\"Solved!\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_continual_backprop(env, policy_net, optimizer, episodes=1000, use_continual=False, render_interval=50):\n",
    "    gamma = 0.99\n",
    "    reward_threshold = 195.0\n",
    "    replacement_rate = 0.3 if use_continual else 0\n",
    "    maturity_threshold = 10\n",
    "\n",
    "    utility1, utility2 = torch.zeros(policy_net.fc1.out_features), torch.zeros(policy_net.fc2.out_features)\n",
    "    age1, age2 = torch.zeros(policy_net.fc1.out_features), torch.zeros(policy_net.fc2.out_features)\n",
    "\n",
    "    def replace_features(layer, utility, age, num_replacements):\n",
    "        lowest_util_indices = utility.argsort()[:num_replacements]\n",
    "        for i in lowest_util_indices:\n",
    "            layer.weight.data[i, :] = torch.randn(layer.weight.shape[1]) * 0.01\n",
    "            utility[i] = 0\n",
    "            age[i] = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for t in range(1, 10000):  # Limit to 10000 steps\n",
    "            if episode % render_interval == 0:\n",
    "                env.render()  # Render every render_interval episodes\n",
    "            \n",
    "            action, log_prob = select_action(policy_net, state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated  # Combine terminated and truncated into a single \"done\" flag\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Compute discounted rewards\n",
    "        discounted_rewards = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            discounted_rewards.insert(0, R)\n",
    "        discounted_rewards = torch.tensor(discounted_rewards)\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n",
    "\n",
    "        # Update policy\n",
    "        loss = -torch.sum(torch.stack(log_probs) * discounted_rewards)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track and update utility for continual backprop\n",
    "        if use_continual:\n",
    "            with torch.no_grad():\n",
    "                # Compute activations for continual backprop\n",
    "                state = torch.from_numpy(state).float()\n",
    "                hidden_output1 = torch.relu(policy_net.fc1(state))\n",
    "                hidden_output2 = torch.relu(policy_net.fc2(hidden_output1))\n",
    "\n",
    "                # Update utility and age\n",
    "                utility1 += torch.abs(hidden_output1)\n",
    "                utility2 += torch.abs(hidden_output2)\n",
    "                age1 += 1\n",
    "                age2 += 1\n",
    "\n",
    "                # Periodic replacement\n",
    "                if episode % maturity_threshold == 0:\n",
    "                    replace_features(policy_net.fc1, utility1, age1, int(replacement_rate * utility1.size(0)))\n",
    "                    replace_features(policy_net.fc2, utility2, age2, int(replacement_rate * utility2.size(0)))\n",
    "\n",
    "        # Logging\n",
    "        total_reward = sum(rewards)\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}\")\n",
    "        if total_reward >= reward_threshold:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    \n",
    "    env.close()  # Close the environment window at the end of training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Continual Backprop:\n",
      "Gravity changed to: 20.0\n",
      "Episode 1/500, Total Reward: 20.0\n",
      "Episode 2/500, Total Reward: 40.0\n",
      "Episode 3/500, Total Reward: 17.0\n",
      "Episode 4/500, Total Reward: 10.0\n",
      "Episode 5/500, Total Reward: 13.0\n",
      "Episode 6/500, Total Reward: 9.0\n",
      "Episode 7/500, Total Reward: 10.0\n",
      "Episode 8/500, Total Reward: 10.0\n",
      "Episode 9/500, Total Reward: 9.0\n",
      "Episode 10/500, Total Reward: 11.0\n",
      "Gravity changed to: 15.0\n",
      "Episode 11/500, Total Reward: 8.0\n",
      "Episode 12/500, Total Reward: 12.0\n",
      "Episode 13/500, Total Reward: 11.0\n",
      "Episode 14/500, Total Reward: 11.0\n",
      "Episode 15/500, Total Reward: 8.0\n",
      "Episode 16/500, Total Reward: 9.0\n",
      "Episode 17/500, Total Reward: 12.0\n",
      "Episode 18/500, Total Reward: 13.0\n",
      "Episode 19/500, Total Reward: 10.0\n",
      "Episode 20/500, Total Reward: 18.0\n",
      "Gravity changed to: 5.0\n",
      "Episode 21/500, Total Reward: 13.0\n",
      "Episode 22/500, Total Reward: 19.0\n",
      "Episode 23/500, Total Reward: 17.0\n",
      "Episode 24/500, Total Reward: 23.0\n",
      "Episode 25/500, Total Reward: 12.0\n",
      "Episode 26/500, Total Reward: 19.0\n",
      "Episode 27/500, Total Reward: 74.0\n",
      "Episode 28/500, Total Reward: 54.0\n",
      "Episode 29/500, Total Reward: 20.0\n",
      "Episode 30/500, Total Reward: 50.0\n",
      "Gravity changed to: 20.0\n",
      "Episode 31/500, Total Reward: 49.0\n",
      "Episode 32/500, Total Reward: 25.0\n",
      "Episode 33/500, Total Reward: 15.0\n",
      "Episode 34/500, Total Reward: 12.0\n",
      "Episode 35/500, Total Reward: 15.0\n",
      "Episode 36/500, Total Reward: 19.0\n",
      "Episode 37/500, Total Reward: 11.0\n",
      "Episode 38/500, Total Reward: 28.0\n",
      "Episode 39/500, Total Reward: 18.0\n",
      "Episode 40/500, Total Reward: 27.0\n",
      "Gravity changed to: 20.0\n",
      "Episode 41/500, Total Reward: 44.0\n",
      "Episode 42/500, Total Reward: 49.0\n",
      "Episode 43/500, Total Reward: 63.0\n",
      "Episode 44/500, Total Reward: 22.0\n",
      "Episode 45/500, Total Reward: 28.0\n",
      "Episode 46/500, Total Reward: 67.0\n",
      "Episode 47/500, Total Reward: 30.0\n",
      "Episode 48/500, Total Reward: 20.0\n",
      "Episode 49/500, Total Reward: 30.0\n",
      "Episode 50/500, Total Reward: 22.0\n",
      "Gravity changed to: 20.0\n",
      "Episode 51/500, Total Reward: 28.0\n",
      "Episode 52/500, Total Reward: 33.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gym/envs/classic_control/cartpole.py:211: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 53/500, Total Reward: 98.0\n",
      "Episode 54/500, Total Reward: 82.0\n",
      "Episode 55/500, Total Reward: 49.0\n",
      "Episode 56/500, Total Reward: 59.0\n",
      "Episode 57/500, Total Reward: 70.0\n",
      "Episode 58/500, Total Reward: 48.0\n",
      "Episode 59/500, Total Reward: 56.0\n",
      "Episode 60/500, Total Reward: 27.0\n",
      "Gravity changed to: 15.0\n",
      "Episode 61/500, Total Reward: 41.0\n",
      "Episode 62/500, Total Reward: 33.0\n",
      "Episode 63/500, Total Reward: 38.0\n",
      "Episode 64/500, Total Reward: 26.0\n",
      "Episode 65/500, Total Reward: 35.0\n",
      "Episode 66/500, Total Reward: 31.0\n",
      "Episode 67/500, Total Reward: 32.0\n",
      "Episode 68/500, Total Reward: 34.0\n",
      "Episode 69/500, Total Reward: 23.0\n",
      "Episode 70/500, Total Reward: 23.0\n",
      "Gravity changed to: 20.0\n",
      "Episode 71/500, Total Reward: 20.0\n",
      "Episode 72/500, Total Reward: 28.0\n",
      "Episode 73/500, Total Reward: 35.0\n",
      "Episode 74/500, Total Reward: 25.0\n",
      "Episode 75/500, Total Reward: 40.0\n",
      "Episode 76/500, Total Reward: 40.0\n",
      "Episode 77/500, Total Reward: 27.0\n",
      "Episode 78/500, Total Reward: 34.0\n",
      "Episode 79/500, Total Reward: 42.0\n",
      "Episode 80/500, Total Reward: 49.0\n",
      "Gravity changed to: 15.0\n",
      "Episode 81/500, Total Reward: 50.0\n",
      "Episode 82/500, Total Reward: 75.0\n",
      "Episode 83/500, Total Reward: 35.0\n",
      "Episode 84/500, Total Reward: 37.0\n",
      "Episode 85/500, Total Reward: 35.0\n",
      "Episode 86/500, Total Reward: 35.0\n",
      "Episode 87/500, Total Reward: 50.0\n",
      "Episode 88/500, Total Reward: 36.0\n",
      "Episode 89/500, Total Reward: 36.0\n",
      "Episode 90/500, Total Reward: 33.0\n",
      "Gravity changed to: 9.8\n",
      "Episode 91/500, Total Reward: 55.0\n",
      "Episode 92/500, Total Reward: 58.0\n",
      "Episode 93/500, Total Reward: 36.0\n",
      "Episode 94/500, Total Reward: 42.0\n",
      "Episode 95/500, Total Reward: 29.0\n",
      "Episode 96/500, Total Reward: 42.0\n",
      "Episode 97/500, Total Reward: 45.0\n",
      "Episode 98/500, Total Reward: 63.0\n",
      "Episode 99/500, Total Reward: 53.0\n",
      "Episode 100/500, Total Reward: 42.0\n",
      "Gravity changed to: 20.0\n",
      "Episode 101/500, Total Reward: 41.0\n",
      "Episode 102/500, Total Reward: 42.0\n",
      "Episode 103/500, Total Reward: 86.0\n",
      "Episode 104/500, Total Reward: 42.0\n",
      "Episode 105/500, Total Reward: 37.0\n",
      "Episode 106/500, Total Reward: 28.0\n",
      "Episode 107/500, Total Reward: 59.0\n",
      "Episode 108/500, Total Reward: 40.0\n",
      "Episode 109/500, Total Reward: 27.0\n",
      "Episode 110/500, Total Reward: 40.0\n",
      "Gravity changed to: 20.0\n",
      "Episode 111/500, Total Reward: 48.0\n",
      "Episode 112/500, Total Reward: 53.0\n",
      "Episode 113/500, Total Reward: 31.0\n",
      "Episode 114/500, Total Reward: 60.0\n",
      "Episode 115/500, Total Reward: 43.0\n",
      "Episode 116/500, Total Reward: 60.0\n",
      "Episode 117/500, Total Reward: 76.0\n",
      "Episode 118/500, Total Reward: 65.0\n",
      "Episode 119/500, Total Reward: 85.0\n",
      "Episode 120/500, Total Reward: 55.0\n",
      "Gravity changed to: 5.0\n",
      "Episode 121/500, Total Reward: 168.0\n",
      "Episode 122/500, Total Reward: 112.0\n",
      "Episode 123/500, Total Reward: 67.0\n",
      "Episode 124/500, Total Reward: 94.0\n",
      "Episode 125/500, Total Reward: 95.0\n",
      "Episode 126/500, Total Reward: 65.0\n",
      "Episode 127/500, Total Reward: 67.0\n",
      "Episode 128/500, Total Reward: 146.0\n",
      "Episode 129/500, Total Reward: 302.0\n",
      "Solved!\n",
      "\n",
      "Training with Standard Backprop:\n",
      "Episode 1/500, Total Reward: 18.0\n",
      "Gravity changed to: 5.0\n",
      "Episode 2/500, Total Reward: 23.0\n",
      "Episode 3/500, Total Reward: 36.0\n",
      "Episode 4/500, Total Reward: 14.0\n",
      "Episode 5/500, Total Reward: 13.0\n",
      "Episode 6/500, Total Reward: 74.0\n",
      "Episode 7/500, Total Reward: 59.0\n",
      "Episode 8/500, Total Reward: 12.0\n",
      "Episode 9/500, Total Reward: 26.0\n",
      "Episode 10/500, Total Reward: 47.0\n",
      "Episode 11/500, Total Reward: 28.0\n",
      "Gravity changed to: 15.0\n",
      "Episode 12/500, Total Reward: 27.0\n",
      "Episode 13/500, Total Reward: 23.0\n",
      "Episode 14/500, Total Reward: 38.0\n",
      "Episode 15/500, Total Reward: 84.0\n",
      "Episode 16/500, Total Reward: 125.0\n",
      "Episode 17/500, Total Reward: 25.0\n",
      "Episode 18/500, Total Reward: 21.0\n",
      "Episode 19/500, Total Reward: 42.0\n",
      "Episode 20/500, Total Reward: 73.0\n",
      "Episode 21/500, Total Reward: 55.0\n",
      "Gravity changed to: 20.0\n",
      "Episode 22/500, Total Reward: 47.0\n",
      "Episode 23/500, Total Reward: 71.0\n",
      "Episode 24/500, Total Reward: 50.0\n",
      "Episode 25/500, Total Reward: 43.0\n",
      "Episode 26/500, Total Reward: 42.0\n",
      "Episode 27/500, Total Reward: 52.0\n",
      "Episode 28/500, Total Reward: 45.0\n",
      "Episode 29/500, Total Reward: 53.0\n",
      "Episode 30/500, Total Reward: 43.0\n",
      "Episode 31/500, Total Reward: 30.0\n",
      "Gravity changed to: 15.0\n",
      "Episode 32/500, Total Reward: 33.0\n",
      "Episode 33/500, Total Reward: 71.0\n",
      "Episode 34/500, Total Reward: 73.0\n",
      "Episode 35/500, Total Reward: 170.0\n",
      "Episode 36/500, Total Reward: 88.0\n",
      "Episode 37/500, Total Reward: 100.0\n",
      "Episode 38/500, Total Reward: 115.0\n",
      "Episode 39/500, Total Reward: 116.0\n",
      "Episode 40/500, Total Reward: 129.0\n",
      "Episode 41/500, Total Reward: 70.0\n",
      "Gravity changed to: 5.0\n",
      "Episode 42/500, Total Reward: 78.0\n",
      "Episode 43/500, Total Reward: 73.0\n",
      "Episode 44/500, Total Reward: 67.0\n",
      "Episode 45/500, Total Reward: 82.0\n",
      "Episode 46/500, Total Reward: 137.0\n",
      "Episode 47/500, Total Reward: 110.0\n",
      "Episode 48/500, Total Reward: 109.0\n",
      "Episode 49/500, Total Reward: 335.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment, policy, and optimizer\n",
    "env = CartPoleNonStationaryEnv()\n",
    "policy_net = PolicyNetwork(input_dim=4, hidden_dim=128, output_dim=2)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n",
    "\n",
    "# Run training with continual backprop\n",
    "print(\"Training with Continual Backprop:\")\n",
    "train_continual_backprop(env, policy_net, optimizer, episodes=500, use_continual=True)\n",
    "\n",
    "# Reset and train with standard backprop\n",
    "policy_net = PolicyNetwork(input_dim=4, hidden_dim=128, output_dim=2)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n",
    "\n",
    "print(\"\\nTraining with Standard Backprop:\")\n",
    "train_continual_backprop(env, policy_net, optimizer, episodes=500, use_continual=False)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAFeCAYAAAAYIxzjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAOwUlEQVR4nO3dS4tk93nA4fdUVV9r7ldNrIw0E0lWjLGJZcuLmAgRCAgCMYIsgiAbLQTZ5RNkk00+glZSNoEsQghZmCQmAluJEDhyJEuxLevaI2luPT09M32prss5WcgSthnNqemuOtWe93lgYAbe7n4Xxcxvqv7nnKKqqioAgLRas14AAJgtMQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOQ6s16A325VVUVEFVFFRFRRjoaxdfXD2Lz6YWxc/SC2Vlfiq3/+N9Fqe6kB7Ff+hmbXqqqMrWsf/fIf/5XYXP0wtlZXIj4LhIiIKGJrdSUOnD4/y1UBuAMxwK6N+r34v3/625qpKq6//2MxALCPOTPA1K2996NZrwDAHYgBdq3VnotjD327frCKqMpy+gsBsCtigF0rWu04fP/v185V5TB2bl5tYCMAdkMMsHtFEYtHTteOlcN+bFx+r4GFANgNMcCuFUUx1tyovx3X339tytsAsFtigD3pLB6M7qlztXNVVFFVzg0A7EdigD2ZWz4UB888XDtX9nsx2L7VwEYA3C0xwJ60Ogsxf+B47dygdyv6N1cb2AiAuyUG2JOiKCLGODrQu34xbnz80+kvBMBdEwPs2Xz3WMwtH64frMpfPssAgP1EDLBn3VMPxvKJs7Vzg95GlIOdBjYC4G6IAfZsbvFgdBa6tXP9W9diuLPZwEYA3A0xwJ4VrVYUrfqX0o0Lb0Zv/WIDGwFwN8QAEzHXPRpFu+YhmFUVVencAMB+IwaYiOMPfzvmu0dr5/pb6xFuPgSwr4gBJmLh4PFodeZr57bXPo5yNGxgIwDGJQaYiFZ7LmKMZxWs/vyVGPW3G9gIgHGJASams7BcO1MOelG53wDAviIGmJj7H386ilbNIcKIGGxcb2AbAMYlBpiYpaNnxvqoYOPK+w1sA8C4xAAT05pbHGvu6k9/MOVNALgbYoCJKlrt2pmdW6sR4cwAwH4hBpioc0/8Zf1QVcVgc33quwAwHjHARC0dv792pqqquPbujxrYBoBxiAEmpiiK6Mwv1Q9WZaz+7L+mvxAAYxEDTFTRmY9D939l1msAcBfEABPVanfiwJlHaueqahTD3kYDGwFQRwwwUUWrE8tHz9TOjfrbsXHZ/QYA9gMxwEQVRRHtMe43MNy+FdfeebWBjQCoIwaYuM7SwVgc490BAPYHMcDEzS0fju6JB2rnymE/RoOdBjYC4E7EABPXnl+KhUMnaucG2zejv7HWwEYA3IkYYOJa7U60OvO1c5uX34sbF95qYCMA7kQMMBVz3aPRXujOeg0AxiAGmIruiQdi8fCp2rnRoBflaNjARgB8ETHAVMwfOBqdxYO1c/1b12I06DWwEQBfRAwwFa3OfLTandq51bdfid71iw1sBMAXEQNMRVEUMdc9EkWrfefBqoyIqpGdALg9McDUHDv/zbEOEQ7721FVggBgVsQAU7N09MxYlxhur30SVTlqYCMAbkcMMDXtheUoivqX2KX//bcoh+5ECDArYoCpKYoi2gtLtXOj/laEjwkAZkYMMFVn/uCpiKKonRsNdpwbAJgRMcBUdU88GBH1MbC5ujL1XQC4PTHAVM11D481d+nH35vyJgB8ETHAVBVRRNGqf5ltr7vxEMCsiAGmqyjiS9/8s7FGXV4IMBtigKnrnjpXO1OVZWxdu9DANgD8JjHA1M0fOFY7U5WjuPb2Kw1sA8BvEgNMVVEUY1xLEBFVGRtX3p/2OgDchhhg6jqL3Tjx6HfqB6tPPy4AoFligKkr2vPRPXG2dq4c9GLn5pUGNgLgV4kBpq7Vbsdc90jt3KC3EbcuvTP9hQD4NWKAfWO0sxmbl50bAGiaGKARi4dPx4H7Hq6dq6KMqnJuAKBJYoBGzHWPxNLRM7Vzw+2NGG5vNLARAJ8RAzSiM78Uc0sHa+d2bl6NnVtXG9gIgM+IAfaV7eufxPbaJ7NeAyAVMUBjlk8+GHPdo7VzVVlGVVUNbARAhBigQd2TD8TCGLcm7m9ej3LYb2AjACLEAA2aWz4c7fnF2rnttY9jNOg1sBEAEWKABhXFWE8piPUPX4/B5vp0lwHgc2KARnVP/V4U7bnauaocOTcA0BAxQKOOPfStaM8t1M7t3LwaEWIAoAligEYtHjoRRatdO7e5uuIJhgANEQM06tMQqD87cPmN70dVDqe/EABigOYtHf/SGFNVVOVo6rsAIAaYgd/5xp+ONbdzc3XKmwAQIQaYgXEeWBQRsXHF44wBmiAGaFxrjEsLIyIuvf7vU94EgAgxwCwURcwtH64dKwc9VxQANEAM0Lii1Yr7H3+6dq4qy+i7EyHA1IkBZqAY69xAVY5i69qFBvYByE0M0LiiKKI1N187V476sfbOqw1sBJCbGGBGivo7EVZV9LdueEYBwJSJAWZi/sCxuO/rf1I7Vw0HMextNLARQF5igJlotedi4eDJ2rn+5nrcuviLBjYCyEsMMBNFqxWtTv39BgZb67Fx+d0GNgLISwwwMwuHT8fSsXGeUxDODQBMkRhgZua7R2Ph0KnauVF/O8phv4GNAHISA8xMZ7Ebc8uHaud66xejv3GtgY0AchIDzEyr3YlWu1M7t3Hp3di69nEDGwHkJAaYqeXjZ6OzeGDWawCkJgaYqe7JB6KzVP9RwXBnI8py1MBGAPmIAWZq/tCJaM8v1s5trX4U5WCngY0A8hEDzFS7Mx9FUXNb4ohY/dkPY7B1o4GNAPIRA8xc98TZKAovRYBZ8TcwM3fk/DeiGONuhMPtW24+BDAF9dd1wZhGo9Gu/rGeP3zfWB8VbK6uxOLJc1EUxW7W+1yr1YpWSwcDfKao/FeLCXnsscfijTfe2NXXfu/v/iIOd+98kLCsIp7867+PwbDc1c/4zIsvvhjPPPPMnr4HwL3EOwNMzGg0iuFwuKuv/eDSenzt/Ok7/6+/qmI4GMZwtLcYKMu9fT3AvcZ7pewL//ifb/7an68N7ov3tr4Wv9h8LC70vhy90XJERBzqLsxiPYB7mncG2BfefP/q57//YPur8eH2V6JXHogy2tEp+vFR+9H4+oH/iC//7on477cuzHBTgHuPdwbYF25u7UREER/1Hom3N78VW+WRKKMTEUUMq4VYH56OV248HX/19B/OelWAe44YYF+oqipWbhyMn2w8EaO4/WWG/WopXus5+AcwaWKAfaE/GMU/fP8nEXGnywaLKIoiFud9ugUwSWKAfaEsq/j5ymrt3HynHefOHJn+QgCJiAH2hSoi1jd7tXMHlubju995dPoLASQiBtg3DrZW45HFH0YRt78PQDsG8UfH/yVOHuk2vBnAvU0MsG9cXLsZb73+z/HQ8v/EQmszihhFRBVF1Y+qfzkerl6Mjy99EheueHohwCQ5icW+sdUbxMqVm/HHrVfjxta78fbqkbi8Poyd7Suxc/21+NdLK7Fy5UZcXd+a9aoA95SxY+C5556b5h7cAy5c2NvNgIajMn7w+odxeW0jVm9sff5rszeY0IafeuGFF+Lll1+e6PcE2K+ef/752pmxY+DZZ5/d0zLc+1566aVYW1vb0/f44NJ6fHBpfTILfYEnnnginnrqqan+DIDfJmPHwOOPPz7NPbgHLC8vz3qFsZw/f97rGeBXOEAIAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEjOUwuZmCeffDLOnTs36zVqnT17dtYrAOwrRVVV1ayXAABmx8cEAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHL/DxUc24evXqdjAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "def visualize_policy_inline(env, policy_net, episodes=5):\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Render the frame and display it inline\n",
    "            frame = env.render()  # No need for mode argument\n",
    "            plt.imshow(frame)\n",
    "            clear_output(wait=True)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            time.sleep(0.02)  # Slow down for visibility\n",
    "\n",
    "            action, _ = select_action(policy_net, state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "    env.close()\n",
    "\n",
    "# Use CartPole with `gymnasium`\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "visualize_policy_inline(env, policy_net)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
